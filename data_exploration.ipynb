# Social Media Sentiment Analysis - Data Exploration
# Jupyter Notebook for comprehensive data exploration and analysis

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
import re
import warnings
warnings.filterwarnings('ignore')

# Set style for better visualizations
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# =============================================================================
# CELL 1: Data Loading and Initial Exploration
# =============================================================================

# Load the sentiment analysis results
# If you don't have results yet, run the main sentiment_analysis.py script first
try:
    df = pd.read_csv('sentiment_analysis_results.csv')
    print("✅ Loaded existing analysis results")
except FileNotFoundError:
    print("⚠️ No existing results found. Generating sample data...")
    # Generate sample data (same as in main script)
    from sentiment_analysis import SentimentAnalyzer
    analyzer = SentimentAnalyzer()
    df = analyzer.create_sample_data(n_samples=1000)
    df = analyzer.analyze_sentiment(df)
    print("✅ Sample data generated and analyzed")

print(f"Dataset shape: {df.shape}")
print(f"Columns: {list(df.columns)}")

# Basic info about the dataset
df.info()

# =============================================================================
# CELL 2: Data Quality Assessment
# =============================================================================

print("=== DATA QUALITY ASSESSMENT ===\n")

# Check for missing values
missing_data = df.isnull().sum()
print("Missing Values:")
print(missing_data[missing_data > 0])

# Check for duplicates
duplicates = df.duplicated().sum()
print(f"\nDuplicate rows: {duplicates}")

# Text length distribution
df['text_length'] = df['text'].str.len()
print(f"\nText Length Statistics:")
print(df['text_length'].describe())

# Check for very short or very long texts
short_texts = (df['text_length'] < 10).sum()
long_texts = (df['text_length'] > 500).sum()
print(f"Very short texts (< 10 chars): {short_texts}")
print(f"Very long texts (> 500 chars): {long_texts}")

# =============================================================================
# CELL 3: Sentiment Distribution Analysis
# =============================================================================

print("\n=== SENTIMENT DISTRIBUTION ANALYSIS ===\n")

# Overall sentiment distribution
sentiment_counts = df['actual_sentiment'].value_counts()
print("Actual Sentiment Distribution:")
print(sentiment_counts)
print(f"Percentages:\n{sentiment_counts/len(df)*100}")

# Compare different methods
comparison_methods = ['textblob_sentiment', 'vader_sentiment', 'actual_sentiment']

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for i, method in enumerate(comparison_methods):
    if method in df.columns:
        counts = df[method].value_counts()
        axes[i].pie(counts.values, labels=counts.index, autopct='%1.1f%%')
        axes[i].set_title(f'{method.replace("_", " ").title()}')

plt.tight_layout()
plt.show()

# =============================================================================
# CELL 4: Text Preprocessing Analysis
# =============================================================================

print("\n=== TEXT PREPROCESSING ANALYSIS ===\n")

def analyze_text_features(text_series, title):
    """Analyze various features of text data"""
    
    print(f"\n--- {title} ---")
    
    # Average text length
    avg_length = text_series.str.len().mean()
    print(f"Average text length: {avg_length:.1f} characters")
    
    # Average word count
    avg_words = text_series.str.split().str.len().mean()
    print(f"Average word count: {avg_words:.1f} words")
    
    # Most common words
    all_text = ' '.join(text_series.dropna().astype(str))
    words = re.findall(r'\b\w+\b', all_text.lower())
    common_words = Counter(words).most_common(10)
    print(f"Most common words: {common_words}")
    
    return common_words

# Analyze original vs cleaned text
original_words = analyze_text_features(df['text'], "Original Text")
if 'cleaned_text' in df.columns:
    cleaned_words = analyze_text_features(df['cleaned_text'], "Cleaned Text")

# =============================================================================
# CELL 5: Sentiment Score Analysis
# =============================================================================

print("\n=== SENTIMENT SCORE ANALYSIS ===\n")

# Statistical summary of sentiment scores
if 'textblob_polarity' in df.columns and 'vader_compound' in df.columns:
    print("TextBlob Polarity Statistics:")
    print(df['textblob_polarity'].describe())
    
    print("\nVADER Compound Statistics:")
    print(df['vader_compound'].describe())
    
    # Correlation between methods
    correlation = df['textblob_polarity'].corr(df['vader_compound'])
    print(f"\nCorrelation between TextBlob and VADER: {correlation:.3f}")
    
    # Distribution plots
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # TextBlob distribution
    axes[0,0].hist(df['textblob_polarity'], bins=30, alpha=0.7, color='blue')
    axes[0,0].set_title('TextBlob Polarity Distribution')
    axes[0,0].set_xlabel('Polarity Score')
    axes[0,0].set_ylabel('Frequency')
    
    # VADER distribution
    axes[0,1].hist(df['vader_compound'], bins=30, alpha=0.7, color='green')
    axes[0,1].set_title('VADER Compound Distribution')
    axes[0,1].set_xlabel('Compound Score')
    axes[0,1].set_ylabel('Frequency')
    
    # Scatter plot
    axes[1,0].scatter(df['textblob_polarity'], df['vader_compound'], alpha=0.6)
    axes[1,0].set_xlabel('TextBlob Polarity')
    axes[1,0].set_ylabel('VADER Compound')
    axes[1,0].set_title('TextBlob vs VADER Scores')
    
    # Box plot by sentiment
    sentiment_scores = df.melt(
        value_vars=['textblob_polarity', 'vader_compound'],
        var_name='method', value_name='score'
    )
    sns.boxplot(data=sentiment_scores, x='method', y='score', ax=axes[1,1])
    axes[1,1].set_title('Score Distribution by Method')
    
    plt.tight_layout()
    plt.show()

# =============================================================================
# CELL 6: Temporal Analysis
# =============================================================================

print("\n=== TEMPORAL ANALYSIS ===\n")

if 'timestamp' in df.columns:
    # Convert timestamp to datetime
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['hour'] = df['timestamp'].dt.hour
    df['day_of_week'] = df['timestamp'].dt.day_name()
    df['date'] = df['timestamp'].dt.date
    
    # Hourly sentiment patterns
    hourly_sentiment = df.groupby(['hour', 'actual_sentiment']).size().unstack(fill_value=0)
    
    plt.figure(figsize=(15, 6))
    hourly_sentiment.plot(kind='bar', stacked=True)
    plt.title('Sentiment Distribution by Hour of Day')
    plt.xlabel('Hour')
    plt.ylabel('Number of Posts')
    plt.legend(title='Sentiment')
    plt.xticks(rotation=0)
    plt.show()
    
    # Daily sentiment trends
    daily_sentiment = df.groupby(['date', 'actual_sentiment']).size().unstack(fill_value=0)
    
    plt.figure(figsize=(15, 6))
    daily_sentiment.plot(kind='area', stacked=True)
    plt.title('Sentiment Trends Over Time')
    plt.xlabel('Date')
    plt.ylabel('Number of Posts')
    plt.legend(title='Sentiment')
    plt.xticks(rotation=45)
    plt.show()
    
    # Peak hours analysis
    positive_by_hour = df[df['actual_sentiment'] == 'positive'].groupby('hour').size()
    peak_positive_hour = positive_by_hour.idxmax()
    print(f"Peak positive sentiment hour: {peak_positive_hour}:00")
    
    negative_by_hour = df[df['actual_sentiment'] == 'negative'].groupby('hour').size()
    peak_negative_hour = negative_by_hour.idxmax()
    print(f"Peak negative sentiment hour: {peak_negative_hour}:00")

# =============================================================================
# CELL 7: Engagement Analysis
# =============================================================================

print("\n=== ENGAGEMENT ANALYSIS ===\n")

if 'likes' in df.columns and 'retweets' in df.columns:
    
    # Calculate total engagement
    df['total_engagement'] = df['likes'] + df['retweets']
    
    # Engagement by sentiment
    engagement_by_sentiment = df.groupby('actual_sentiment').agg({
        'likes': ['mean', 'median', 'std'],
        'retweets': ['mean', 'median', 'std'],
        'total_engagement': ['mean', 'median', 'std']
    }).round(2)
    
    print("Engagement Statistics by Sentiment:")
    print(engagement_by_sentiment)
    
    # Visualization
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # Likes by sentiment
    sns.boxplot(data=df, x='actual_sentiment', y='likes', ax=axes[0,0])
    axes[0,0].set_title('Likes Distribution by Sentiment')
    
    # Retweets by sentiment
    sns.boxplot(data=df, x='actual_sentiment', y='retweets', ax=axes[0,1])
    axes[0,1].set_title('Retweets Distribution by Sentiment')
    
    # Total engagement by sentiment
    sns.boxplot(data=df, x='actual_sentiment', y='total_engagement', ax=axes[1,0])
    axes[1,0].set_title('Total Engagement by Sentiment')
    
    # Correlation heatmap
    correlation_cols = ['textblob_polarity', 'vader_compound', 'likes', 'retweets', 'text_length']
    if all(col in df.columns for col in correlation_cols):
        corr_matrix = df[correlation_cols].corr()
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1,1])
        axes[1,1].set_title('Correlation Matrix')
    
    plt.tight_layout()
    plt.show()
    
    # High engagement posts analysis
    high_engagement = df[df['total_engagement'] > df['total_engagement'].quantile(0.9)]
    print(f"\nTop 10% engagement posts sentiment distribution:")
    print(high_engagement['actual_sentiment'].value_counts(normalize=True))

# =============================================================================
# CELL 8: Text Length and Complexity Analysis
# =============================================================================

print("\n=== TEXT LENGTH AND COMPLEXITY ANALYSIS ===\n")

# Text length by sentiment
length_by_sentiment = df.groupby('actual_sentiment')['text_length'].agg([
    'mean', 'median', 'std', 'min', 'max'
]).round(2)

print("Text Length Statistics by Sentiment:")
print(length_by_sentiment)

# Word count analysis
df['word_count'] = df['text'].str.split().str.len()

plt.figure(figsize=(15, 5))

# Text length distribution
plt.subplot(1, 3, 1)
for sentiment in df['actual_sentiment'].unique():
    data = df[df['actual_sentiment'] == sentiment]['text_length']
    plt.hist(data, bins=20, alpha=0.6, label=sentiment)
plt.xlabel('Text Length (characters)')
plt.ylabel('Frequency')
plt.title('Text Length Distribution by Sentiment')
plt.legend()

# Word count distribution
plt.subplot(1, 3, 2)
for sentiment in df['actual_sentiment'].unique():
    data = df[df['actual_sentiment'] == sentiment]['word_count']
    plt.hist(data, bins=20, alpha=0.6, label=sentiment)
plt.xlabel('Word Count')
plt.ylabel('Frequency')
plt.title('Word Count Distribution by Sentiment')
plt.legend()

# Box plot comparison
plt.subplot(1, 3, 3)
sns.boxplot(data=df, x='actual_sentiment', y='text_length')
plt.title('Text Length by Sentiment (Box Plot)')

plt.tight_layout()
plt.show()

# =============================================================================
# CELL 9: Model Performance Analysis
# =============================================================================

print("\n=== MODEL PERFORMANCE ANALYSIS ===\n")

def calculate_accuracy(actual, predicted):
    """Calculate accuracy between actual and predicted sentiments"""
    return (actual == predicted).mean()

# Calculate accuracies
if all(col in df.columns for col in ['actual_sentiment', 'textblob_sentiment', 'vader_sentiment']):
    
    tb_accuracy = calculate_accuracy(df['actual_sentiment'], df['textblob_sentiment'])
    vader_accuracy = calculate_accuracy(df['actual_sentiment'], df['vader_sentiment'])
    
    print(f"TextBlob Accuracy: {tb_accuracy:.3f}")
    print(f"VADER Accuracy: {vader_accuracy:.3f}")
    
    # Confusion matrices
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # TextBlob confusion matrix
    from sklearn.metrics import confusion_matrix
    import pandas as pd
    
    tb_cm = confusion_matrix(df['actual_sentiment'], df['textblob_sentiment'])
    tb_cm_df = pd.DataFrame(tb_cm, 
                           index=df['actual_sentiment'].unique(),
                           columns=df['textblob_sentiment'].unique())
    sns.heatmap(tb_cm_df, annot=True, fmt='d', cmap='Blues', ax=axes[0])
    axes[0].set_title(f'TextBlob Confusion Matrix (Acc: {tb_accuracy:.3f})')
    axes[0].set_ylabel('Actual')
    axes[0].set_xlabel('Predicted')
    
    # VADER confusion matrix
    vader_cm = confusion_matrix(df['actual_sentiment'], df['vader_sentiment'])
    vader_cm_df = pd.DataFrame(vader_cm,
                              index=df['actual_sentiment'].unique(),
                              columns=df['vader_sentiment'].unique())
    sns.heatmap(vader_cm_df, annot=True, fmt='d', cmap='Greens', ax=axes[1])
    axes[1].set_title(f'VADER Confusion Matrix (Acc: {vader_accuracy:.3f})')
    axes[1].set_ylabel('Actual')
    axes[1].set_xlabel('Predicted')
    
    plt.tight_layout()
    plt.show()
    
    # Classification reports
    from sklearn.metrics import classification_report
    
    print("\nTextBlob Classification Report:")
    print(classification_report(df['actual_sentiment'], df['textblob_sentiment']))
    
    print("\nVADER Classification Report:")
    print(classification_report(df['actual_sentiment'], df['vader_sentiment']))

# =============================================================================
# CELL 10: Advanced Insights and Recommendations
# =============================================================================

print("\n=== ADVANCED INSIGHTS AND RECOMMENDATIONS ===\n")

# Most challenging examples (where methods disagree)
if all(col in df.columns for col in ['textblob_sentiment', 'vader_sentiment', 'actual_sentiment']):
    
    # Find disagreements
    disagreements = df[df['textblob_sentiment'] != df['vader_sentiment']]
    print(f"Cases where TextBlob and VADER disagree: {len(disagreements)} ({len(disagreements)/len(df)*100:.1f}%)")
    
    if len(disagreements) > 0:
        print("\nSample disagreement cases:")
        print(disagreements[['text', 'textblob_sentiment', 'vader_sentiment', 'actual_sentiment']].head())
    
    # Identify most confident predictions
    df['confidence_score'] = abs(df['textblob_polarity']) + abs(df['vader_compound'])
    high_confidence = df.nlargest(10, 'confidence_score')
    low_confidence = df.nsmallest(10, 'confidence_score')
    
    print(f"\nHighest confidence predictions (avg score: {high_confidence['confidence_score'].mean():.3f}):")
    print(high_confidence[['text', 'textblob_sentiment', 'vader_sentiment', 'confidence_score']].head(3))
    
    print(f"\nLowest confidence predictions (avg score: {low_confidence['confidence_score'].mean():.3f}):")
    print(low_confidence[['text', 'textblob_sentiment', 'vader_sentiment', 'confidence_score']].head(3))

# Recommendations for improvement
print("\n=== RECOMMENDATIONS FOR MODEL IMPROVEMENT ===")
print("""
1. DATA QUALITY:
   - Increase dataset size for better model training
   - Include more domain-specific data
   - Balance sentiment classes if heavily skewed

2. PREPROCESSING:
   - Implement more sophisticated text cleaning
   - Handle sarcasm and context better
   - Consider emoji sentiment

3. MODEL ENHANCEMENT:
   - Combine TextBlob and VADER using ensemble methods
   - Fine-tune models on domain-specific data
   - Implement deep learning approaches (BERT, RoBERTa)

4. EVALUATION:
   - Use cross-validation for robust performance metrics
   - Include precision, recall, and F1-scores
   - Analyze performance across different text lengths and topics

5. DEPLOYMENT:
   - Implement real-time sentiment monitoring
   - Add confidence intervals to predictions
   - Create feedback loops for continuous improvement
""")

print("\n✅ Data exploration complete!")
print(f"📊 Analyzed {len(df)} posts with {df.shape[1]} features")
print("📋 Check the visualizations and insights above for detailed analysis")
